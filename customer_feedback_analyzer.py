# -*- coding: utf-8 -*-
"""Customer_FeedBack_Analyzer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1klTyqU08SPeDPlGaym_CoZZJR06DMTFW

### ***1. Introduction***
 
Feedback is one of the prominent things in mar-
keting by which we can understand the brand’s
product value in the market. We want to use NLP
ideas like sentiment analysis to automate the under-
standing of customers intentions, the key aspects of
a product that customers care for and their underly-
ing intentions and reactions behind it. Our model
is a prototype of the idea which leverages NLP
techniques to understand the intentions behind the
feedback received. Word embeddings are effective
intermediate representations for capturing seman-
tic regularities between words in natural language
processing (NLP) tasks. We propose sentiment-
aware word embedding for emotional classification,
which consists of integrating sentiment evidence
within the emotional embedding component of a
term vector. We take advantage of the multiple
types of emotional knowledge, just as the existing emotional lexicon, to build emotional word vectors
to represent emotional information.
"""



def trial(x):
  if(x==1):
    return 1;
  else:
    return x*trial(x-1)/(x-1)
print(trial(20))

def modulus(x):
  if(x<0):
    return -1*x
  else:
    return x

print(modulus(20))
print(modulus(-20))

#---- Library used in sentiment Analysis
!pip install vaderSentiment

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
tf.test.gpu_device_name()

# Commented out IPython magic to ensure Python compatibility.
#--- Import libraries
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer
import numpy as np
import pandas as pd
from pandas import DataFrame
import gzip

import matplotlib.pyplot as plt
import seaborn as sns
import calendar

import warnings
warnings.filterwarnings("ignore")

from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

from sklearn.neighbors import NearestNeighbors
from sklearn import neighbors
from scipy.spatial.distance import cosine
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin

from nltk.stem.porter import PorterStemmer

from wordcloud import WordCloud, STOPWORDS

# % matplotlib inline
import nltk
nltk.download('stopwords')

def return_dict(file):
  yield eval(file)

def parse(path):
  g = gzip.open(path, 'rb')
  return map(return_dict, g)
  """
  for l in g:
    yield eval(l)

  """
def getDF(path):
  i = 0
  df = {}
  for d in parse(path):
    df[i] = d
    i += 1
  return pd.DataFrame.from_dict(df, orient='index')


#paths
electronics_path = "https://www.cs.odu.edu/~hpendyal/courses/cs620/project/dataset/Electronics_5.json"

#meta data
electronicsMetaPath = "https://www.cs.odu.edu/~hpendyal/courses/cs620/project/dataset/meta_Electronics.json"

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #---- Reading Electronics data
# Electronics = pd.read_json(electronics_path, lines=True)
# print('-> Total reviews : ',Electronics.reviewText.count())
# print("Data Loading completed in ")

Electronics.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #--- Reading Electronics MetaData
# electronics_meta = pd.read_json(electronicsMetaPath,lines=True)
# print('--> Number of products : ',electronics_meta.title.count())
# print("Data Loading completed in ")

#--- Removing 'imUrl' as we are not using it for any purpose(Those are basically for images)
electronics_meta = electronics_meta[['asin','description','categories','title','price','salesRank','related','brand']]
electronics_meta.head()

Electronics.info()

"""### ***2. Exploratory Data Analysis***"""

# Commented out IPython magic to ensure Python compatibility.
# #--- split the date into seperate columns of date month and year
# %%time
# pklPath = '/content/drive/My Drive/ODU/CourseWork/NLPProject/Electronics_split.pkl'
# Electronics = pd.read_pickle(pklPath)

f, axes = plt.subplots(2,2, figsize=(12,8))
#--- Yearly Reviews
yearly = Electronics.groupby(['year'])['reviewerID'].count().reset_index()
yearly = yearly.rename(columns={'reviewerID':'no_of_reviews'})
yearChart = sns.lineplot(x='year',y='no_of_reviews',data=yearly, ax = axes[0,0])
yearChart.set_title('No of reviews over years')

#--- Monthly Reviews
monthly = Electronics.groupby(['month'])['reviewerID'].count().reset_index()
monthly['month'] = monthly['month'].apply(lambda x : calendar.month_name[x])
monthly = monthly.rename(columns={'reviewerID':'no_of_reviews'})
monthChart = sns.barplot(x='month',y='no_of_reviews',data=monthly, ax = axes[0,1])
monthChart.set_title('No of reviews over month')
monthChart.set_xticklabels(monthChart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')

#-- Getting overall ratings for every electronics products
#Electronics['overall'].value_counts().plot(kind='bar')
sns.countplot(x = 'overall', data = Electronics, ax = axes[1,0] ).set_title('Overall Reviews')

#--- helpfulness of review.
helpfulness = Electronics[['helpful','asin']]
helpfulness[['helpfulVotes','totalVotes']] = pd.DataFrame(helpfulness.helpful.values.tolist(), index=helpfulness.index)
helpfulness = helpfulness.drop(['helpful'], axis = 1)
#--- calculating helpfulness Percentage
helpfulness['percentage'] = (helpfulness.helpfulVotes/helpfulness.totalVotes)*100
helpfulness = helpfulness.fillna(0)
final_helpfullness = helpfulness.groupby(pd.cut(helpfulness.percentage,np.arange(0,101,10))).count()
final_helpfullness = final_helpfullness.rename(columns={'percentage':'count'})
final_helpfullness = final_helpfullness.reset_index()
helpfullnessChart = sns.barplot(x='percentage',y='count',data=final_helpfullness, ax = axes[1,1])
helpfullnessChart.set_title('helpfullness of reviews ranked by percentage')
helpfullnessChart.set_xticklabels(helpfullnessChart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')

f.tight_layout()

"""***Rating Trend over the years***
- There is an increasing trend for number of ratings given by the users to products on Amazon which indicates that a greater number of users started using the Amazon e-commerce site for online shopping and a greater number of users started giving feedback on the products purchased from 2000 to 2014. 

***Distribution of overall ratings***
- Many users have given a rating of 5 to products followed by 4 and 3 whereas very few users have given a low rating of 1 or 2.

"""

f, axes = plt.subplots(1,2, figsize=(12,8))
#---- Distribution of word reviews by length
Electronics_reviews = Electronics[['asin','reviewerID','reviewerName','reviewText','summary','overall']]
Electronics_reviews['reviewLength'] = Electronics_reviews['reviewText'].apply(lambda x: len(x.split()))
#Electronics_reviews.head()
reviews_word_length = Electronics_reviews.groupby(pd.cut(Electronics_reviews.reviewLength, np.arange(0,1000,100))).count()
reviews_word_length = reviews_word_length.rename(columns={'reviewLength':'count'})
reviews_word_length = reviews_word_length.reset_index()
#print(reviews_word_length)
reviewLengthChart = sns.barplot(x='reviewLength',y='count',data=reviews_word_length, ax = axes[0])
reviewLengthChart.set_title('Distribution of Reviews by word length')
reviewLengthChart.set_xticklabels(reviewLengthChart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')

#--- Products, Distribution of prices of products
#---max price of the produc = 999.99
#--- Splitting the data into range
price10=electronics_meta[(electronics_meta.price > 0) & (electronics_meta.price <= 10)]
price50=electronics_meta[(electronics_meta.price > 10) & (electronics_meta.price <= 50)]
price100=electronics_meta[(electronics_meta.price > 50) & (electronics_meta.price <= 100)]
price200=electronics_meta[(electronics_meta.price > 100) & (electronics_meta.price <= 200)]
price500=electronics_meta[(electronics_meta.price > 200) & (electronics_meta.price <= 500)]
price1000=electronics_meta[(electronics_meta.price > 500) & (electronics_meta.price <= 1000)]
#--- creating a list to create a dataframe further
priceDist = []
priceDist.append(('[0,10]',price10.shape[0]))
priceDist.append(('[11,50]',price50.shape[0]))
priceDist.append(('[51,100]',price100.shape[0]))
priceDist.append(('[101,200]',price200.shape[0]))
priceDist.append(('[201,500]',price500.shape[0]))
priceDist.append(('[501,1000]',price1000.shape[0]))
#--- Creating DataFrame
priceDist = pd.DataFrame(priceDist,columns=['range','count'])
priceDistChart = sns.barplot(x='range',y='count',data=priceDist, ax = axes[1])
priceDistChart.set_title('Price Distribution in range 0 - 1000 Dollars')
priceDistChart.set_xticklabels(priceDistChart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')

f.tight_layout()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# f, axes = plt.subplots(1,2, figsize=(12,8))
# #--- Distribution of product Price vs overall Ratings of products by users
# #--- got data of 63001 products
# averageProductRatings = Electronics.groupby('asin')['overall'].mean().reset_index()
# productPrices = electronics_meta[['asin','price']]
# price_vs_rating = pd.merge(averageProductRatings, productPrices, on='asin', how='inner') #--- in this way we will be dealing with only common products
# price_vs_ratingChart = sns.scatterplot(x='overall', y='price', data=price_vs_rating, ax = axes[0])
# price_vs_ratingChart.set_title('Product Price vs Overall Rating')
# 
# #--- Distribution of length of reviews vs overall rating
# reviewLength_vs_Rating = Electronics_reviews[['asin','reviewLength','overall']]
# reviewLength_vs_Rating = sns.scatterplot(x='overall', y='reviewLength', data=reviewLength_vs_Rating, ax = axes[1])
# reviewLength_vs_Rating.set_title('Review Length vs Overall Rating')
# 
# f.tight_layout()

"""from the above plot we can see that most of the reviews come under 3 - 5 for a price range of 200 - 400. and rating and review length are relted to each other."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #--- mean median and mode of overall ratings
# f = plt.figure(figsize=(18,10))
# #---mean
# stat_reviews_yearly = Electronics.groupby(['year'])['overall'].mean().reset_index()
# stat_reviews_yearly = stat_reviews_yearly.rename(columns={'overall':'mean_overall'})
# #---median
# median_yearly = Electronics.groupby(['year'])['overall'].median().reset_index()
# stat_reviews_yearly['median_overall'] = median_yearly['overall']
# #--- plotting the values
# sns.lineplot(x='year',y='mean_overall',data=stat_reviews_yearly, label = 'Mean')
# sns.lineplot(x='year',y='median_overall',data=stat_reviews_yearly, label = 'Median')
# f.tight_layout()

"""Looking at above plot, we can infer that over the years 2000 to 2004, the mean rating of the products has reduced and then increaased at a slow rate till 2014 but still is much lower than before 2000. Median of ratings given to products remains at 5 from 2000 to 2014 except for years 2004 and 2006"""

# #---mode
Electronics.groupby(['year'])['overall'].value_counts()

electronics_meta.head()

#--- get the unique products in products list
len(Electronics['reviewerID'].unique())

reviewLength = Electronics_reviews[['asin','reviewLength']]
print(reviewLength.count())
print(helpfulness.count())

#--- 9995 unique brands in 491192 products
productCategories = pd.DataFrame(electronics_meta.categories.values.tolist(), index = electronics_meta.categories.index)
productCategories.loc[:, productCategories.isna().any()] #--- to select those columns (containing at least one NaN value)
#--- removing the null columns
productCategories = productCategories.drop([1,2,3,4,5,6,7,8,9,10,11,12],axis = 1)
productCategories = productCategories.rename(columns={0:'categories'})
#--- removing the null columns
productCategories = pd.DataFrame(productCategories.categories.values.tolist(), index = productCategories.categories.index)
#--- Splitting the columns to get more products
productCategories = productCategories.rename(columns={0:'mainCategory',1:'subCategory1',2:'subCategory2',3:'subCategory3',4:'subCategory4',5:'subCategory5'})
#--- Remove Duplicates
productCategories_ND = productCategories.drop_duplicates()
productCategories_ND.head()
electronicsCategories = productCategories_ND[productCategories_ND['mainCategory'] == 'Electronics']
electronicsCategories = electronicsCategories.reset_index()
electronicsCategories
#electronicsCategories.subCategory1.unique()
#categoryChart = sns.scatterplot(x='mainCategory',y='subCategory1', size='mainCategory', data=productCategories)

#--- Distribution of number of reviews written by each user
f = plt.figure(figsize=(18,10))
userReviews = Electronics[['reviewerID','asin']]
userReviews = userReviews.groupby(['reviewerID']).count().reset_index()
userReviews = userReviews.sort_values('asin',ascending = False)
userReviews = userReviews.rename(columns={'asin':'no of reviews'})
print(userReviews.head())

userReviews1 = userReviews.groupby('no of reviews')['reviewerID'].count().reset_index()
userReviews1 = userReviews1.rename(columns={'reviewerID':'count'})
print()
print(userReviews1.head())
userReviewChart = sns.barplot(x = 'no of reviews',y = 'count',data = userReviews1)
userReviewChart.set_title('Reviews Written by each user')
userReviewChart.set_xticklabels(userReviewChart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')
f.tight_layout()

f = plt.figure(figsize=(12,10))
userReviews2 = userReviews1.groupby(pd.cut(userReviews1['no of reviews'],np.arange(0,200,10))).sum()
userReviews2 = userReviews2.rename(columns={'no of reviews':'range of reveiws'})
userReviews2 = userReviews2.reset_index()
userReviewChart = sns.barplot(x='no of reviews',y='count',data=userReviews2)
userReviewChart.set_title('Reviews Written by each user in a range')
userReviewChart.set_xticklabels(userReviewChart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')
f.tight_layout()

"""on an average each user gives 5 reviews and most reviewed user has given 430 reviews for all the different products, facts and figures are shown below in recommender system as it gives us a better understanding of the user.

### ***3. Data Cleaning***

Data preprocessing and cleaning is an important step before any text processing  task, in this step, we will remove the punctuations, stopwords and normalize the reviews as much as possible.

#### ***3.1 Handling Duplicate data***

There exist a lot of duplicates wherein the different products is reviewed by same user at the same time
The product ID may be different but the product is similar with different variant.
"""

Electronics.duplicated(subset={"reviewerID","reviewerName","reviewTime","reviewText"}).value_counts()

ElectronicsFinal = Electronics[Electronics['reviewerID'] == 'AMO214LNFCEI4']
#Electronics.head()
ElectronicsFinal

"""In the above data we can see that 'Amazon Customer' gave the multiple product reviews [ 'B001MSVPM6' , 'B007B31IAK' , 'B00DSTUVHW' ] at the same time which is not possible ethically, the product were same but different flavours hence counted as multiple products, this can be seen from below.

"""

electronics_meta.loc[electronics_meta.asin.isin(ElectronicsFinal.asin.values.tolist())]

#Deleting all the duplicates having the same userID, Profile, NameTime and Text all in the same column.
ElectronicsFinal =  Electronics.drop_duplicates(subset={"reviewerID","reviewerName","reviewTime","reviewText"},keep="first")

size_diff = ElectronicsFinal['asin'].size/Electronics['asin'].size
print("%.1f %% reduction in data after deleting duplicates"%((1-size_diff)*100))
print("Size of data",ElectronicsFinal['asin'].size," rows ")

#---- Data is cleaned and no Duplicates are there

"""#### ***3.2 Text Preprocessing***

As the review is mostly text data, we might need to clean the data to gain some useful insights from the data 
"""

#--- HTML Tag Removal
import re #Regex (Regular Expr Operations)
#string = r"sdfsdfd" :- r is for raw string as Regex often uses \ backslashes(\w), so they are often raw strings(r’\d’)
########Function to remove html tags from data
def striphtml(data):
    p = re.compile('<.*?>')#Find this kind of pattern
    #print(p.findall(data))#List of strings which follow the regex pattern
    return p.sub('',data) #Substitute nothing at the place of strings which matched the patterns

striphtml('<a href="foo.com" class="bar">My Name is  <b>Aditya Vardhan gara!</b></a><>')

#--- Punctuation removal
########Function to remove All the punctuations from the text
def strippunc(data):
    p = re.compile(r'[?|!|\'|"|#|.|,|)|(|\|/|~|%|*]')
    return p.sub('',data)
strippunc("Aditya *?~,,,( Vardhan)#")

#--- StopWords

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

stop = stopwords.words('english') #All the stopwords in English language
#excluding some useful words from stop words list as we doing sentiment analysis
excluding = ['against','not','don', "don't",'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't",
             'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 
             'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't",'shouldn', "shouldn't", 'wasn',
             "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
stop = [words for words in stop if words not in excluding]
print(stop)

from nltk.stem import SnowballStemmer
snow = SnowballStemmer('english') #initialising the snowball stemmer
print("Stem/Root words of the some of the words using SnowBall Stemmer:")
print(snow.stem('tasty'))
print(snow.stem('tasteful'))
print(snow.stem('tastiest'))
print(snow.stem('delicious'))
print(snow.stem('amazing'))
print(snow.stem('amaze'))
print(snow.stem('initialize'))
print(snow.stem('fabulous'))
print(snow.stem('Honda City'))
print(snow.stem('unpleasant'))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #pre-processing output for one Review
# def preprocessText(text, stem=False):
#   filtered_sentence = []
#   final_string = []
#   #print(text)
#   text = striphtml(text) #--- remove HTML Tags
#   text = strippunc(text) #--- remove Punctuation
#   for w in text.split(): #--- isalpha() methods returns “True” if all characters in the string are alphabets, Otherwise, It returns “False”.
#     if(w.isalpha() and (len(w) > 2)):  #--- Check is value is not numeric and has length > 2
#       if(w.lower() not in stop):       #--- Check if it is a stopword
#         if stem : 
#           s = (snow.stem(w.lower())).encode('utf8') #--- Stemming the word using snowball stemmer
#         else:
#           s = (w.lower()).encode('utf8') #--- Stemming the word using snowball stemmer
#         filtered_sentence.append(s)
#       else:
#         continue
#     else:
#       continue
#   cleanedText = b" ".join(filtered_sentence) # string of cleaned words
#   final_string.append(cleanedText)
#   return final_string
# 
# print('---- Uncleaned Text ----')
# print(ElectronicsFinal.reviewText[0])
# print('---- Cleaned Text ----')
# print(preprocessText(ElectronicsFinal.reviewText[0],True))
# print()
# print('---- Cleaned Text (without Stemming)----')
# print(preprocessText(ElectronicsFinal.reviewText[0]))
# print()
# 
# print("Preprocessing completed in ")
# 
#

"""***Note*** : Data is already preprocessed and cleaned so we will be reading the cleaned data from the .pkl file"""

Electronics_main = Electronics.copy(deep = True)
#pklpath = "/content/drive/My Drive/ODU/CourseWork/NLPProject/Electronics_Cleaned.pkl" #--- Stemmed Reviews
pklpath = "/content/drive/My Drive/ODU/CourseWork/NLPProject/Electronics_Cleaned_NO_STEM.pkl" #--- NON Stemmed Reviews
Electronics = pd.read_pickle(pklpath)
Electronics.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #--- WordCloud
# from wordcloud import WordCloud, STOPWORDS
# stopwords = set(STOPWORDS)
# def show_wordcloud(data, title = None):
#   wordcloud = WordCloud(background_color='white', stopwords=stopwords, max_words=200, max_font_size=40, scale=3, random_state=1).generate(str(data)) # chosen at random by flipping a coin; it was heads
#   fig = plt.figure(1, figsize=(12, 10))
#   plt.axis('off')
#   if title: 
#     fig.suptitle(title, fontsize=20)
#     fig.subplots_adjust(top=2.3)
#   plt.imshow(wordcloud)
#   plt.show()
# 
# show_wordcloud(Electronics['cleanedReview'])
# print()
# print()
# print("Word Cloud processing completed in ")

"""### ***4. Sentiment Analysis***

Sentiment Analysis is the automated process of understanding the sentiment or opinion of a given text. This machine learning tool can provide insights by automatically analyzing product reviews and separating them into tags: Positive, Neutral, Negative.

#### ***4.1. Classifying reviews based on sentiment using VaderSentiment***
In this part, I have used a prebuilt library [VaderSentiment](https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f) which is used in predicting the sentiment of a review based on the lexicon arrangement of the words in a review. Also different statergies to actually predict the sentiment for each review by classifying it into Positive, Neutral and Negative reviews. 

- Using naive bayes classifier to generate probabilities for each document 
- based on generated sentiment score on each document, we can classify the review into 3 categories(positive negative and neutral)
"""

#---Function to calculate sentiments using Naive Bayes Analyzer
def NaiveBaiyes_Sentimental(sentence):
    blob = TextBlob(sentence, analyzer=NaiveBayesAnalyzer())
    NaiveBayes_SentimentScore=blob.sentiment.classification
    return NaiveBayes_SentimentScore

# VADER sentiment analysis tool for getting Compound score.
def sentimental(sentence):
    analyzer = SentimentIntensityAnalyzer()
    vs = analyzer.polarity_scores(sentence)
    score=vs['compound']
    return score

# VADER sentiment analysis tool for getting pos, neg and neu.
def sentimental_Score(sentence):
    analyzer = SentimentIntensityAnalyzer()
    vs = analyzer.polarity_scores(sentence)
    score=vs['compound']
    if score >= 0.5:
        return 'pos'
    elif (score > -0.5) and (score < 0.5):
        return 'neu'
    elif score <= -0.5:
        return 'neg'

#---- sample run on classifying the sentance
#sentence = 'there is no negative about this product It actually works with zero faults.' #---- Worst Case -- POS but Classified as NEG
#sentence = 'this product is absolutely great.' #---- Best Case -- POS
#sentence = 'this product is absolutely great but with some faults.' #---- Best Case -- NEU
sentence = 'this product actually works with zero faults.' #---- Worst Case -- POS but classified as NEG
sent = sentimental_Score(sentence)
print('The Sentence : \n',sentence)
print('The Sentence is of " ',sent,' " Sentiment.')

"""From the above we can see that some times the library generates a false sentiment. So have implemented a custom Classifier to get the Sentiment of each review.


Sentiment value is calculated for each review and stored in the new column 'Sentiment_Score' of DataFrame 'Electronics'.

As we have generated sentiment scores for all the reviews we can now see what are the most repeated words for positive and negative reviews using a word cloud.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sentimentAnalysisPath = "/content/drive/My Drive/ODU/CourseWork/CS - 620 | F19 | Intro to Data Science/Project/SentimentAnalysis.pkl"
# PositiveReviewsPath = "/content/drive/My Drive/ODU/CourseWork/CS - 620 | F19 | Intro to Data Science/Project/SentimentAnalysis-positive.pkl"
# NegativeReviewsPath = "/content/drive/My Drive/ODU/CourseWork/CS - 620 | F19 | Intro to Data Science/Project/SentimentAnalysis-negative.pkl"
# NeutralReviewsPath = "/content/drive/My Drive/ODU/CourseWork/CS - 620 | F19 | Intro to Data Science/Project/SentimentAnalysis-neutral.pkl"
# 
# positive_data = pd.read_pickle(PositiveReviewsPath)
# negative_data = pd.read_pickle(NegativeReviewsPath)
# neutral_data = pd.read_pickle(NeutralReviewsPath)

# Commented out IPython magic to ensure Python compatibility.
# #---- Positive Reviews
# %%time
# show_wordcloud(positive_data['cleanedReview'])
# print()
# print("Word Cloud processing completed in ")

# Commented out IPython magic to ensure Python compatibility.
# #---- Negative Reviews
# %%time
# show_wordcloud(negative_data['cleanedReview'])
# print()
# print("Word Cloud processing completed in ")

# Commented out IPython magic to ensure Python compatibility.
# #---- Neutral Reviews
# %%time
# show_wordcloud(neutral_data['cleanedReview'])
# print()
# print("Word Cloud processing completed in ")

Electronics_df = pd.read_json(electronics_path, lines=True)
Electronics_df.head()

"""#### ***4.2 Building a custom classifier for sentimental analysis of Reviews***

As I want to study sentiment analysis, I wanted to buld a custom classifier that predicts the sentiment of the reviews based on the review summary. 

I have chosen naive bayes and logistic regression as models to classify the sentiment of the reviews.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# dataset_cleaned = pd.read_pickle(sentimentAnalysisPath)
# dataset_cleaned.head()

"""
ignoring the reviews having overall score equal to 3. If the overall score is above 3, then the label for it will be set to Positive else it will be set to Negative"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# reviews = dataset_cleaned[dataset_cleaned['overall']!=3]
# reviews = reviews[['overall','summary','helpfulVotes','totalVotes']]
# print('length of required data : ',len(reviews.overall))

reviews.head()

"""sentiment column depicts the numeric score of being positive or negative usefulScore column depicts the boolean value of total number of votes"""

#%%time
reviews["sentiment"] = reviews["overall"].apply(lambda score: "positive" if score > 3 else "negative")
reviews["usefulScore"] = (reviews["helpfulVotes"]/reviews["totalVotes"]).apply(lambda n: "useful" if n > 0.8 else "useless")
reviews.head(5)

"""Rows having overall score equal to 5"""

reviews[reviews.overall == 5].head(5)

#Rows having overall score equal to 1
reviews[reviews.overall == 1].head(5)

"""Assign new dimension to each word and give the word counts"""

regEx = re.compile('[^a-z]+')
def cleanReviews(reviewText):
    reviewText = reviewText.lower()
    reviewText = regEx.sub(' ', reviewText).strip()
    return reviewText

reviews["summaryClean"] = reviews["summary"].apply(cleanReviews)

train, test = train_test_split(reviews, test_size=0.2)
print("%d items in training data, %d in test data" % (len(train), len(test)))

"""Using TfidfTransformer().fit_transform to fit the train and test data"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# countVector = CountVectorizer(min_df = 1, ngram_range = (1, 4))
# X_train_counts = countVector.fit_transform(train["summaryClean"])
# 
# #applying tfidf to term frequency
# tfidf_transformer = TfidfTransformer()
# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
# 
# X_new_counts = countVector.transform(test["summaryClean"])
# X_test_tfidf = tfidf_transformer.transform(X_new_counts)
# 
# y_train = train["sentiment"]
# y_test = test["sentiment"]
# 
# prediction = dict()

"""We use below methods to classify the sentiment of the data.
> 1. ***Multinomial Naïve Bayes learning method***
> 2. ***Logistic regression learning method***
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #--- Naive Bayes
# model = MultinomialNB().fit(X_train_tfidf, y_train)
# prediction['Multinomial'] = model.predict(X_test_tfidf)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #--- Logistic Regression
# logreg = LogisticRegression(C=1e5)
# logreg_result = logreg.fit(X_train_tfidf, y_train)
# prediction['Logistic'] = logreg.predict(X_test_tfidf)

"""#### ***4.3 Model Testing***

***Testing the model for few custom reviews***

As we have seen that the predefined library was not generating the sentiments we need, I build a custom classifier, lets test it for few reviews and see how it classifes the sentiment of the review. 
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def testSentiments(model, testData):
#     testCounts = countVector.transform([testData])
#     testTfidf = tfidf_transformer.transform(testCounts)
#     result = model.predict(testTfidf)[0]
#     probability = model.predict_proba(testTfidf)[0]
#     print("Sample estimated as %s: negative prob %f, positive prob %f" % (result.upper(), probability[0], probability[1]))
# 
# sentence1 = 'this product is absolutely great.' #---- Best Case -- POS
# sentence3 = 'this product is absolutely great but with some faults.' #---- Best Case -- NEU
# sentence2 = 'this product actually works with zero faults.' #---- Worst Case -- POS but classified as NEG
# sentence4 = 'there is no negative about this product It actually works with zero faults.' #---- Worst Case -- POS but Classified as NEG
# 
# 
# testSentiments(logreg, sentence1)
# testSentiments(logreg, sentence2)
# testSentiments(logreg, sentence3)
# testSentiments(logreg, sentence4)

"""#### ***4.4 Evaluating the Results***
- Using ROC curve
    
    Using the curve we can see which model has better performance and use that model for testing purpose. 
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# def formatt(x):
#     if x == 'negative':
#         return 0
#     return 1
# vfunc = np.vectorize(formatt)
# 
# plt.figure(figsize=(20,10)) 
# cmp = 0
# colors = ['b', 'g', 'y', 'm', 'k']
# for model, predicted in prediction.items():
#     false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))
#     roc_auc = auc(false_positive_rate, true_positive_rate)
#     plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))
#     cmp += 1
# 
# plt.title('Classifiers comparaison with ROC')
# plt.legend(loc='lower right')
# plt.plot([0,1],[0,1],'r--')
# plt.xlim([-0.1,1.2])
# plt.ylim([-0.1,1.2])
# plt.ylabel('True Positive Rate')
# plt.xlabel('False Positive Rate')
# plt.show()

"""we can see in the above figure that the logistic regression clearly outperforms other model. so lets see the accuracy of logistic regression

Visualize the accuracy, recall and f1-score for Logistic Regression
"""

print(metrics.classification_report(y_test, prediction['Logistic'], target_names = ["positive", "negative"]))

accuracy_score(y_test, prediction['Logistic'])

"""***Plotting the confusion Matrix***"""

def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues, labels=["positive", "negative"]):
    plt.imshow(matrix, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
# Compute confusion matrix
matrix = confusion_matrix(y_test, prediction['Logistic'])
np.set_printoptions(precision=2)
plt.figure(figsize=(7,6))
plot_confusion_matrix(matrix)    

matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]
plt.figure(figsize=(7,6))
plot_confusion_matrix(matrix_normalized, title='Normalized confusion matrix')
plt.show()

"""Getting the words that classify the best and worst features.

We us the logistic model as it gives the best results
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# wordfeaturesPath = '/content/drive/My Drive/ODU/CourseWork/NLPProject/wordFeatures.csv'
# features = countVector.get_feature_names()
# feature_coefs = pd.DataFrame(
#     data = list(zip(features, logreg_result.coef_[0])),
#     columns = ['feature', 'coefficient'])
# 
# feature_coefs = feature_coefs.sort_values(by='coefficient')
# feature_coefs.to_csv(wordfeaturesPath)
# print(feature_coefs.head())

"""Sentiment analysis is performed using logistic regression and Naive Bayes. Also, the user behavior is analyzed and the popular words used by the users are determined.

### ***5. Recommendation system.***

Recommender systems help customers by suggesting probable list of products from which they can easily select the right one. They make customers aware of new and/or similar products available for purchase by providing comparable costs, features, delivery times etc.

#### ***5.1 Getting the most reviewed USER***

ADLVFFE4VBT8 -- This reviewer has more than 430 reviews ('MOST REVIEWED PERSON')

name - A. Dent "Aragorn"
"""

MRP = dataset_cleaned[dataset_cleaned['reviewerID'] == 'ADLVFFE4VBT8']
MRP_YCount = MRP.groupby('year')['reviewerID'].count().reset_index()
MRP_YCount=MRP_YCount.rename(columns={'reviewerID':'No_Of_Reviews'})
MRP.head()

"""Grouping on 'Year' which we got in previous step and getting the count of reviews.

***DISTRIBUTION OF REVIEWS OVER THE YEARS FOR 'A. Dent "Aragorn"'***
"""

MRP_YCount.plot(x="year",y="No_Of_Reviews",kind="bar",title="Number of Reviews Over Years by 'A. Dent 'Aragorn'", figsize=(18,10))
plt.show()

def ReviewCategory(score):
    if score >= 4:
        return 'pos'
    elif (score <= 2) & (score > 0):
        return 'neg'
    else:
        return 'neu'

MRP['category']=MRP['overall'].apply(lambda x: ReviewCategory(x))

"""Grouping on 'Category' which we got in previous step and getting the count of reviews."""

CategoryCount=MRP.groupby('category')['reviewerID'].count().reset_index()
CategoryCount=CategoryCount.rename(columns={'reviewerID':'Count'})
CategoryCount.plot(x="category",y="Count",kind="bar",title="Category V/S No. of Reviews", figsize=(18,10))
plt.show()

"""***WORDCLOUD OF ALL IMPORTANT WORDS USED IN 'A. Dent "Aragorn"' REVIEWS ON AMAZON***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# show_wordcloud(MRP['cleanedReview'])
# print()
# print("Word Cloud processing completed in ")

"""***NUMBER OF DISTINCT PRODUCTS REVIEWED BY 'A. Dent "Aragorn"' ON AMAZON***"""

len(MRP['asin'].unique())

"""> The user has always reviewed different products

***PRODUCTS REVIEWED BY 'A. Dent "Aragorn"'***
"""

em = electronics_meta[['asin','title']]
Products_Reviewed=pd.merge(MRP,em,on="asin",how="left")
Products_Reviewed=Products_Reviewed[['asin','title','overall']]
Products_Reviewed.head()

csvpath_MRP = '/content/drive/My Drive/ODU/CourseWork/NLPProject/Products_Reviewed.csv'
Products_Reviewed.to_csv(csvpath_MRP)

Product_List=Products_Reviewed["asin"].tolist()
#Product_List

# to make a multilevel list values flat
def make_flat(arr):
    res = []
    for l in arr:
        if isinstance(l, list):
            res.extend(make_flat(l)) 
        else:
            res.append(l)
    return res

electronics_meta[electronics_meta['asin'] == 'B00127VF68'].categories

"""***POPULAR SUB-CATEGORY FOR 'A. Dent "Aragorn"'***"""

print(electronicsCategories.subCategory1.unique())
#print(electronicsCategories.subCategory2.unique())
#print(electronicsCategories.subCategory3.unique())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import collections
# 
# pop_catg=[]
# for p in Product_List:
#   pop_catg.append(make_flat(electronics_meta[electronics_meta['asin'] == p].categories))
#   pop_catg= make_flat(pop_catg)
# print(pop_catg)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# top_catg=collections.Counter(pop_catg).most_common(11)
# top_catg
# top5=[]
# for x in top_catg[1:6]:
#     top5.append(x[0])
# top5=pd.DataFrame(top5,columns=['Sub-Category'])
# print(top5.head())

"""***PRICE RANGE IN WHICH 'A. Dent "Aragorn"' SHOPS***"""

Price_Range_dataset=electronics_meta[['asin','title','price']]
Price_Range_dataset=pd.merge(MRP,Price_Range_dataset,on="asin",how="left")
print('Average Price for A. Dent "Aragorn" shopped : ',Price_Range_dataset.price.mean())
print('Minimum Price for A. Dent "Aragorn" shopped : ',Price_Range_dataset.price.min())
print('Maximum Price for A. Dent "Aragorn" shopped : ',Price_Range_dataset.price.max())

electronics_meta.head()
#electronics_meta.loc[2].related

"""#### ***5.2 Building a Recommendation System***

***Product based Collaborative Filtering (Item Based)***


"""

dataset_cleaned.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# count = dataset_cleaned.groupby("asin", as_index=False).count()
# mean = dataset_cleaned.groupby("asin", as_index=False).mean()
# dfMerged = pd.merge(dataset_cleaned, count, how='right', on=['asin'])
# dfMerged.head()

#rename column
dfMerged["totalReviewers"] = dfMerged["reviewerID_y"]
dfMerged["overallScore"] = dfMerged["overall_x"]
dfMerged["summaryReview"] = dfMerged["summary_x"]

dfNew = dfMerged[['asin','summaryReview','overallScore',"totalReviewers"]]

dfMerged = dfMerged.sort_values(by='totalReviewers', ascending=False)
dfCount = dfMerged[dfMerged.totalReviewers >= 100]
dfCount.head()

"""***Grouping all the summary Reviews by product ID***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# product_review_path = "/content/drive/My Drive/ODU/CourseWork/NLPProject/ProductReviewSummary.csv"
# dfProductReview = dataset_cleaned.groupby("asin", as_index=False).mean()
# ProductReviewSummary = dfCount.groupby("asin")["summaryReview"].apply(list)
# ProductReviewSummary = pd.DataFrame(ProductReviewSummary)
# ProductReviewSummary.to_csv(product_review_path)
# dfProductReview.head()

dfProductReview.head()

ProductReviewSummary.head()

df3 = pd.read_csv(product_review_path)
df3 = pd.merge(df3, dfProductReview, on="asin", how='inner')
df3 = df3[['asin','summaryReview','overall']]
df3.head()

"""***Text Cleaning - Summary column***"""

#function for tokenizing summary
regEx = re.compile('[^a-z]+')
def cleanReviews(reviewText):
    reviewText = reviewText.lower()
    reviewText = regEx.sub(' ', reviewText).strip()
    return reviewText

#reset index and drop duplicate rows
df3["summaryClean"] = df3["summaryReview"].apply(cleanReviews)
df3 = df3.drop_duplicates(['overall'], keep='last')
df3 = df3.reset_index()

reviews = df3["summaryClean"] 
countVector = CountVectorizer(max_features = 300, stop_words='english') 
transformedReviews = countVector.fit_transform(reviews) 

dfReviews = DataFrame(transformedReviews.A, columns=countVector.get_feature_names())
dfReviews = dfReviews.astype(int)

#save 
dfreviews_path = "/content/drive/My Drive/ODU/CourseWork/NLPProject/dfReviews.csv"
dfReviews.to_csv(dfreviews_path)

# First let's create a dataset called X
X = np.array(dfReviews)
 # create train and test
tpercent = 0.9
tsize = int(np.floor(tpercent * len(dfReviews)))
dfReviews_train = X[:tsize]
dfReviews_test = X[tsize:]
#len of train and test
lentrain = len(dfReviews_train)
lentest = len(dfReviews_test)

# KNN classifier to find similar products
print(lentrain)
print(lentest)

neighbor = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(dfReviews_train)

# Let's find the k-neighbors of each point in object X. To do that we call the kneighbors() function on object X.
distances, indices = neighbor.kneighbors(dfReviews_train)

#find most related products
for i in range(lentest):
    a = neighbor.kneighbors([dfReviews_test[i]])
    related_product_list = a[1]

    first_related_product = [item[0] for item in related_product_list]
    first_related_product = str(first_related_product).strip('[]')
    first_related_product = int(first_related_product)
    second_related_product = [item[1] for item in related_product_list]
    second_related_product = str(second_related_product).strip('[]')
    second_related_product = int(second_related_product)
    
    print ("Based on product reviews, for ", df3["asin"][lentrain + i] ," average rating is ",df3["overall"][lentrain + i])
    print ("The first similar product is ", df3["asin"][first_related_product] ," average rating is ",df3["overall"][first_related_product])
    print ("The second similar product is ", df3["asin"][second_related_product] ," average rating is ",df3["overall"][second_related_product])
    print ("-----------------------------------------------------------")

"""***Predicting Review Score***"""

df5_train_target = df3["overall"][:lentrain]
df5_test_target = df3["overall"][lentrain:lentrain+lentest]
df5_train_target = df5_train_target.astype(int)
df5_test_target = df5_test_target.astype(int)

n_neighbors = 3
knnclf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
knnclf.fit(dfReviews_train, df5_train_target)
knnpreds_test = knnclf.predict(dfReviews_test)

print(classification_report(df5_test_target, knnpreds_test))

print('Accuracy Score : ',accuracy_score(df5_test_target, knnpreds_test))
print('Mean Square Error : ',mean_squared_error(df5_test_target, knnpreds_test))

"""### ***6. Conclusion and Future work***

- Performed Exploratory Data Analysis on dataset to come up with insights such as rating, review trends over the years, range of ratings every year, helpfullness of the reviews, price range of products.

- Feature Extraction using TF-IDF vectorization for both the problems.

- ***Sentiment Analysis*** : Given a product and its reviews and ratings along with helpfullness score, predict the sentiment of review, whether the reveiw is 'positive' or 'negative'. 

  Logistic Regression is comes out to be the best model to classify the sentiment of reviews.

- ***Recommender System*** : Given a product and its reviews and ratings along with helpfullness score, recommended the next best product based on the review summary and the overall rating. 

- For recommendations, Analysing most reveiewed user help us to understand user behaviour. Applied K-Nearest Neighbours with k=2 is used to get 2 nearest products based on review summary and overall review score, recomemnd the products to the user.

#### 6.1 ***Future Work***

- Customize the recommender for remaining product categories in Amazon Review dataset would be the next step. This recommender makes use of ratings, summary of review given by users. Making use of review text given by users would be interesting as a further step.
- Analysing whole reviews and Predicting the sentiment 
- Finding more patterns in the data to make more accurate recommendations.
- apply advanced techniques like BERT, wor2Vec to generate more presonalized recommendations.
"""